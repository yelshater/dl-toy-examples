{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters initialization\n",
    "\n",
    "IMAGE_SIZE = 28*28\n",
    "NUM_CLASSES = 10\n",
    "MAX_EPOCH = 5\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # 1 input image : 28*28 , output is 10 classes\n",
    "        self.linear = nn.Linear(in_features=IMAGE_SIZE, out_features=NUM_CLASSES,bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset\n",
    "ROOT = \"data/\"\n",
    "\n",
    "#transforms.ToTensor() transforms the PIL.Image.Image format to pytorch Tensor \n",
    "\n",
    "train_MINST = MNIST(ROOT+\"MNIST\", train=True, download=True,transform=transforms.ToTensor())\n",
    "test_MINST = MNIST(ROOT+\"MNIST\", train=False, download=True,transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_MINST,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_MINST,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#Before iterating all the training set, verfiy the dimensions of the training and target tensors of a random example\n",
    "(images, labels) = list(train_data_loader)[0]\n",
    "# images shape now is [BATCH_SIZE, 1, 28, 28], it should be reshaped ot [BATCH_SIZE,IMAGE_SIZE]\n",
    "images = images.reshape(-1,IMAGE_SIZE)\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "assert images.shape[0] == BATCH_SIZE\n",
    "assert images.shape[1] == IMAGE_SIZE\n",
    "assert labels.shape[0] == BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How PyTorch calculates Cross Entropy\n",
    "\n",
    "Pytorch calculates the softmax of the output first before calculating the cross entropy.\n",
    "\n",
    "For example, assume we have four classes and the output of the model is something like [0,0,0,1] and the actual target is [3]. Calculating the cross entropy directly should equal to zero (1*log(1)). \n",
    "\n",
    "However, PyTorch converts the outputs to probabilities that sums to 1 using softmax: \n",
    "softmax([0,0,0,1]) =  [0.1749,0.1749,0.1749,0.4754]\n",
    "Then, apply the cross entropy formula -log(0.4754) = 0.7437\n",
    "\n",
    "Reference: https://stackoverflow.com/questions/49390842/cross-entropy-in-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7437)\n"
     ]
    }
   ],
   "source": [
    "output = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)\n",
    "target = Variable(torch.LongTensor([3]))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "del loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple logistic regression model\n",
    "$$y_{m*c} = x_{m*n} * w_{n*c} + b_{m*c}$$\n",
    "where $m$ is the number of examples (i.e. the number of the training images), $n$ is the number of features (i.e. the image size) and $c$ is the number of the classes\n",
    "\n",
    "Note that $b$ have the same values that are broadcasted to all the examples $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(IMAGE_SIZE, NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(), lr=LEARNING_RATE)  \n",
    "\n",
    "for epoch in range(0,MAX_EPOCH):\n",
    "    #train_data_loader is already partitioned to multiple batches, each batch with size = LENGTH(TRAINING_SET)/BATCH_SIZE\n",
    "    for batch_index, (images, labels) in enumerate(train_data_loader):\n",
    "        images = Variable(images.reshape(-1, 28*28),requires_grad=True)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        #reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #calculate predictions (y^)\n",
    "        predictions = lr_model(images)\n",
    "        \n",
    "        #calculate Cross Entropy loss between labels(y) and predictions(y^)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        #Backward propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the learned weights vector w\n",
    "w = list(lr_model.parameters())[0]\n",
    "\n",
    "assert w.shape[0] == NUM_CLASSES\n",
    "assert w.shape[1] == IMAGE_SIZE\n",
    "\n",
    "# get the learned bias vector b\n",
    "b = list(lr_model.parameters())[1]\n",
    "\n",
    "assert b.shape[0] == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct classified test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model accuracy\n",
    "correct_instances = 0\n",
    "total_test_examples = 0\n",
    "for images, labels in test_data_loader:\n",
    "    images = Variable(images.reshape(-1, 28*28))\n",
    "    predictions_output = lr_model(images)\n",
    "    _, predicted_classes = torch.max(predictions_output.data, 1)\n",
    "    total_test_examples = total_test_examples + len(images)\n",
    "    correct_instances = correct_instances + (predicted_classes == labels).sum()\n",
    "\n",
    "print('Number of correct classified test images: %d %%' % (100 * correct_instances / total_test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
