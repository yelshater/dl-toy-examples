{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint,EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from keras.layers import LSTM,Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "path = get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "#just sample for now \n",
    "text = text[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_words = [w for w in text.replace('\\n', ' \\n ').split(' ') if w.strip() != '' or w == '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 52\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 3179\n",
      "Ignoring words with frequency < 3\n",
      "Unique words after ignoring: 468\n"
     ]
    }
   ],
   "source": [
    "MIN_WORD_FREQUENCY=3\n",
    "# Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in text_in_words:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "words = set(text_in_words)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 9108\n",
      "Remaining sequences: 1643\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "SEQUENCE_LEN=4\n",
    "STEP = 1\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored+1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 1610\n",
      "Size of test set = 33\n"
     ]
    }
   ],
   "source": [
    "sentences, next_words, sentences_test, next_words_test = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yehiaelshater/anaconda/envs/mypython3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/yehiaelshater/anaconda/envs/mypython3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))\n",
    "if dropout > 0:\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size,generate_labels=True):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            if generate_labels:\n",
    "                y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        if generate_labels:\n",
    "            yield x, y\n",
    "        else:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yehiaelshater/anaconda/envs/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "323/323 [==============================] - 6s 18ms/step - loss: 15.7666 - acc: 0.0180 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 15.8087 - acc: 0.0192 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 15.8140 - acc: 0.0186 - val_loss: 15.6760 - val_acc: 0.0286\n",
      "Epoch 4/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 8.5002 - acc: 0.0669 - val_loss: 4.7831 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 5.8354 - acc: 0.1084 - val_loss: 4.9678 - val_acc: 0.0571\n",
      "Epoch 6/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 5.9770 - acc: 0.1300 - val_loss: 5.1312 - val_acc: 0.0571\n",
      "Epoch 7/30\n",
      "323/323 [==============================] - 5s 14ms/step - loss: 5.8537 - acc: 0.1610 - val_loss: 5.1784 - val_acc: 0.0857\n",
      "Epoch 8/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 5.5918 - acc: 0.1820 - val_loss: 5.2474 - val_acc: 0.2000\n",
      "Epoch 9/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 5.1616 - acc: 0.2136 - val_loss: 5.6859 - val_acc: 0.1714\n",
      "Epoch 10/30\n",
      "323/323 [==============================] - 5s 16ms/step - loss: 4.9347 - acc: 0.2186 - val_loss: 5.3619 - val_acc: 0.2571\n",
      "Epoch 11/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 4.7443 - acc: 0.2186 - val_loss: 6.0419 - val_acc: 0.0857\n",
      "Epoch 12/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 4.4080 - acc: 0.2365 - val_loss: 5.5027 - val_acc: 0.1714\n",
      "Epoch 13/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 4.1672 - acc: 0.2607 - val_loss: 5.9533 - val_acc: 0.2000\n",
      "Epoch 14/30\n",
      "323/323 [==============================] - 5s 15ms/step - loss: 3.9362 - acc: 0.2824 - val_loss: 6.8328 - val_acc: 0.1714\n",
      "Epoch 15/30\n",
      "323/323 [==============================] - 5s 16ms/step - loss: 3.6144 - acc: 0.3214 - val_loss: 7.1715 - val_acc: 0.1143\n",
      "Epoch 16/30\n",
      "323/323 [==============================] - 5s 16ms/step - loss: 3.3453 - acc: 0.3529 - val_loss: 7.0879 - val_acc: 0.1429\n",
      "Epoch 17/30\n",
      "323/323 [==============================] - 5s 16ms/step - loss: 3.1077 - acc: 0.3759 - val_loss: 8.5775 - val_acc: 0.0857\n",
      "Epoch 18/30\n",
      "323/323 [==============================] - 6s 18ms/step - loss: 3.0360 - acc: 0.4037 - val_loss: 8.1221 - val_acc: 0.1714\n",
      "Epoch 19/30\n",
      "323/323 [==============================] - 6s 18ms/step - loss: 2.8522 - acc: 0.4272 - val_loss: 7.3967 - val_acc: 0.1429\n",
      "Epoch 20/30\n",
      "323/323 [==============================] - 5s 16ms/step - loss: 2.6785 - acc: 0.4502 - val_loss: 8.4580 - val_acc: 0.1143\n",
      "Epoch 21/30\n",
      "323/323 [==============================] - 6s 18ms/step - loss: 2.4630 - acc: 0.4941 - val_loss: 7.8588 - val_acc: 0.2000\n",
      "Epoch 22/30\n",
      "323/323 [==============================] - 6s 18ms/step - loss: 2.3784 - acc: 0.5084 - val_loss: 8.3156 - val_acc: 0.1714\n",
      "Epoch 23/30\n",
      "323/323 [==============================] - 4s 14ms/step - loss: 2.3033 - acc: 0.5387 - val_loss: 8.4703 - val_acc: 0.2286\n",
      "Epoch 24/30\n",
      "323/323 [==============================] - 6s 18ms/step - loss: 2.0161 - acc: 0.5616 - val_loss: 8.5394 - val_acc: 0.2286\n",
      "Epoch 25/30\n",
      "323/323 [==============================] - 7s 22ms/step - loss: 1.9090 - acc: 0.6124 - val_loss: 8.6498 - val_acc: 0.1143\n",
      "Epoch 26/30\n",
      "323/323 [==============================] - 7s 20ms/step - loss: 1.8865 - acc: 0.6142 - val_loss: 9.5390 - val_acc: 0.1143\n",
      "Epoch 27/30\n",
      "323/323 [==============================] - 8s 24ms/step - loss: 1.9208 - acc: 0.6223 - val_loss: 9.6927 - val_acc: 0.1143\n",
      "Epoch 28/30\n",
      "323/323 [==============================] - 8s 24ms/step - loss: 1.7281 - acc: 0.6514 - val_loss: 10.2184 - val_acc: 0.1429\n",
      "Epoch 29/30\n",
      "323/323 [==============================] - 7s 23ms/step - loss: 1.7582 - acc: 0.6539 - val_loss: 10.2564 - val_acc: 0.1429\n",
      "Epoch 30/30\n",
      "323/323 [==============================] - 5s 16ms/step - loss: 1.6414 - acc: 0.6793 - val_loss: 9.8653 - val_acc: 0.1429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2a1daef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(words),\n",
    "    SEQUENCE_LEN,\n",
    "    MIN_WORD_FREQUENCY\n",
    ")\n",
    "\n",
    "on_epoch_end = 30\n",
    "#checkpoint = ModelCheckpoint(file_path, monitor='val_loss', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "#callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "#callbacks_list = [print_callback]\n",
    "\n",
    "optimizer = RMSprop(lr=0.05)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "epochs=on_epoch_end,\n",
    "#callbacks=callbacks_list\n",
    "validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),  validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_seed(vocabulary, seed):\n",
    "    \"\"\"Validate that all the words in the seed are part of the vocabulary\"\"\"\n",
    "    print(\"\\nValidating that all the words in the seed are part of the vocabulary: \")\n",
    "    seed_words = seed.split(\" \")\n",
    "    valid = True\n",
    "    for w in seed_words:\n",
    "        print(w, end=\"\")\n",
    "        if w in vocabulary:\n",
    "            print(\" ✓ in vocabulary\")\n",
    "        else:\n",
    "            print(\" ✗ NOT in vocabulary\")\n",
    "            valid = False\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'the', 'same', 'time']\n",
      " when this world still \n",
      " the work want all the will thought of this \n",
      " metaphysical is not believe in the \n",
      " let good is"
     ]
    }
   ],
   "source": [
    "quantity = 25 #quantity of words to generate\n",
    "sentence = sentences[random.randint(0,len(sentences))]\n",
    "print(sentence)\n",
    "for i in range(quantity):\n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x_pred[0, t, word_indices[word]] = 1\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, 2)\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    sentence = sentence[1:]\n",
    "    #print(sentence)\n",
    "    sentence.append(next_word)\n",
    "\n",
    "    print(\" \"+next_word, end=\"\")\n",
    "    #print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred[0,1,word_indices['last']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26, 7], [48, 38], [30, 38], [48, 41], [22], [31], [11, 41], [16, 48], [11, 38], [47, 11, 7, 22]]\n",
      "[[26  7  0  0]\n",
      " [48 38  0  0]\n",
      " [30 38  0  0]\n",
      " [48 41  0  0]\n",
      " [22  0  0  0]\n",
      " [31  0  0  0]\n",
      " [11 41  0  0]\n",
      " [16 48  0  0]\n",
      " [11 38  0  0]\n",
      " [47 11  7 22]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "        'nice work',\n",
    "\t\t'Great effort',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
